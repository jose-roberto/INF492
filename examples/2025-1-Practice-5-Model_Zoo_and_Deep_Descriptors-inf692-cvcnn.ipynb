{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b2609f-a9dc-4ab8-9676-73b735dc1fd1",
   "metadata": {},
   "source": [
    "## Model Zoo | Classify your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c807d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b372b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_image_show ( image ) :\n",
    "    \n",
    "#     plt.figure(figsize = (10,10))\n",
    "    plt.imshow(image, aspect='auto')\n",
    "    plt.axis('off')\n",
    "    plt.title('My image.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea9830-8859-4aad-8461-e3a5dcbfbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def my_tensor_image_show ( image , label=None ):\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    if label is None :\n",
    "        plt.title('Image in tensor format.')\n",
    "    else :\n",
    "        plt.title(f'Image in tensor format | Class: {label:2d}')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f69dc-7279-47f7-923b-2467e1d737d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_raw_tensor_image_show ( image , label=None ):\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    if label is None :\n",
    "        plt.title('Image in tensor format.')\n",
    "    else :\n",
    "        plt.title(f'Image in tensor format | Class: {label:2d}')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a676ab-39c3-40db-83b7-d344478e510f",
   "metadata": {},
   "source": [
    "### Load image using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac75c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "dataset_path = '/homeLocal/praticas-cv-cnn/datasets/'\n",
    "name = dataset_path + '/car.jpeg'\n",
    "\n",
    "\n",
    "image = cv2.imread( name )\n",
    "image = cv2.cvtColor( image , cv2.COLOR_BGR2RGB )\n",
    "my_image_show(image)\n",
    "\n",
    "print(type(image))\n",
    "print(image.shape)\n",
    "\n",
    "pil_image = Image.fromarray(image)\n",
    "print(type(pil_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49160b9-e32d-4f39-a324-21f3752f0449",
   "metadata": {},
   "source": [
    "### Load image using PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9e3d8-5d2d-4992-ab71-073b44fc0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/homeLocal/praticas-cv-cnn/datasets/'\n",
    "name = dataset_path + '/car.jpeg'\n",
    "\n",
    "pil_image = Image.open( name )\n",
    "my_image_show(pil_image)\n",
    "\n",
    "print(type(pil_image))\n",
    "print(pil_image.size, pil_image.getbands())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b405c3-3470-4946-b6a0-f2da32e2d09c",
   "metadata": {},
   "source": [
    "### Load image using Decode_image from torchvision io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a3f60-bf7e-4840-83c6-4a87dcf4e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import decode_image\n",
    "\n",
    "dataset_path = '/homeLocal/praticas-cv-cnn/datasets/'\n",
    "name = dataset_path + '/car.jpeg'\n",
    "\n",
    "img = decode_image(name)\n",
    "\n",
    "my_raw_tensor_image_show(img)\n",
    "\n",
    "# In this case, img is already a tensor. I just renamed it to pil_image to maintaing consistency along the code.\n",
    "pil_image = img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd36dc0-2579-49b1-b6f3-70c006b8382a",
   "metadata": {},
   "source": [
    "### Process the data and classify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ffdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "import torch\n",
    "\n",
    "my_transform = AlexNet_Weights.IMAGENET1K_V1.transforms()\n",
    "model_input = my_transform(pil_image) \n",
    "\n",
    "my_tensor_image_show(model_input)\n",
    "print(model_input.shape)\n",
    "\n",
    "# model_input = my_transform(pil_image) \n",
    "model_input = model_input.unsqueeze_(0)\n",
    "print(model_input.shape)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device(\"cuda:0\")    \n",
    "else:\n",
    "    my_device = torch.device(\"cpu\")\n",
    "\n",
    "my_device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Running on {my_device.type}.\")\n",
    "\n",
    "model = torchvision.models.alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "model = model.to(my_device)\n",
    "model_input = model_input.to(my_device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(model_input)\n",
    "    \n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the image class\n",
    "\n",
    "prediction = output.squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = AlexNet_Weights.IMAGENET1K_V1.meta[\"categories\"][class_id]\n",
    "print(f\"Class: {category_name} with probability {100 * score:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c5733-7a66-4940-b997-b15758a5f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the first top-k image class probabilities\n",
    "\n",
    "prediction = output.squeeze(0).softmax(0)\n",
    "  \n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(prediction, 10)\n",
    "\n",
    "for i in range(top5_prob.size(0)):\n",
    "    category_name = AlexNet_Weights.IMAGENET1K_V1.meta[\"categories\"][top5_catid[i]]\n",
    "    print( f'Class: {category_name} with probability {100*top5_prob[i].item():5.2f}%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad859a1-798b-437b-ba8f-6b3a7b7fb7d2",
   "metadata": {},
   "source": [
    "## Model Zoo | Describle your own data using a deep descriptor\n",
    "\n",
    "Visit https://docs.pytorch.org/vision/stable/models.html to a list of available models.\n",
    "\n",
    "Click in the desired model for more details. Then click in the GitHub link of the model. \n",
    "\n",
    "Create a Class that extends the model class, and overwrite the atribute on the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import AlexNet\n",
    "\n",
    "class AlexNetDescriptor(AlexNet):\n",
    "    def __init__(self):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(256 * 6 * 6, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # torch.nn.Dropout(),\n",
    "            # torch.nn.Linear(4096, 4096),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_descriptor = AlexNetDescriptor()\n",
    "model_descriptor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b17903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the manner I explained during the class. However, in the new version of PyTorch,\n",
    "# the same result can be achieved using the bellow cell.\n",
    "\n",
    "descriptor_dict = model_descriptor.state_dict()\n",
    "pretrained_dict = model.state_dict()\n",
    "\n",
    "# 1. filter out keys not presented in the new model\n",
    "keys_dict = { k: v for k, v in pretrained_dict.items() if k in descriptor_dict }\n",
    "# 2. overwrite entries in the existing state dict\n",
    "descriptor_dict.update(keys_dict) \n",
    "# 3. load the new state dict\n",
    "model_descriptor.load_state_dict(descriptor_dict)\n",
    "print(descriptor_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b2bf7-89db-4d16-aa78-a9fe9e974d77",
   "metadata": {},
   "source": [
    "#### You can load the weights using only the below code, instead of running the more complex code presented in the above cells. Using this approach, it is not necessary to load the dictionary of both models, compare the keys, filter out the keys, and load the new dictionary in the descriptor model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b76fa2-40f6-40c1-b989-99858d950fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import AlexNet\n",
    "\n",
    "class AlexNetDescriptor(AlexNet):\n",
    "    def __init__(self):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(256 * 6 * 6, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            # torch.nn.ReLU(inplace=True),\n",
    "            # torch.nn.Linear(4096, 1000),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model_descriptor = AlexNetDescriptor()\n",
    "\n",
    "# Directly load weights\n",
    "model_descriptor.load_state_dict(torchvision.models.alexnet(weights=AlexNet_Weights.IMAGENET1K_V1).state_dict(), strict=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95db380",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = my_transform(pil_image) \n",
    "model_input = model_input.unsqueeze_(0)\n",
    "print(model_input.shape)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    my_device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Running on {my_device.type}.\")\n",
    "\n",
    "model_descriptor = model_descriptor.to(my_device)\n",
    "model_input = model_input.to(my_device)\n",
    "\n",
    "model_descriptor.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_descriptor(model_input)\n",
    "    \n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
